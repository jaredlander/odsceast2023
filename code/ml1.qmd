---
title: ML in R
author: Jared P. Lander
date: today
date-format: long
format:
    html: 
      toc: true
      number-sections: true
      code-link: true
---

# Data

```{r}
data(credit_data, package='modeldata')

credit <- tibble::as_tibble(credit_data)
credit
```

```{r}
library(dplyr)
```

```{r}
fake_customers <- credit |> 
    select(-Status) |> 
    slice_sample(n=10)
fake_customers
```

Split the data

```{r}
library(rsample)

set.seed(37)
data_split <- initial_split(
    credit, 
    prop=0.8,
    strata='Status'
)
data_split

train <- training(data_split)
test <- testing(data_split)

train
test
```

# EDA

Now we use our favorite packages, `{dplyr}` and `{ggplot2}`

```{r}
library(ggplot2)
ggplot(train, aes(x=Status)) + geom_bar()
```

There are a lot more good than bad, could be a problem

```{r}
train |> count(Status)
```

```{r}
ggplot(train, aes(x=Amount, y=Age, color=Status)) + 
    geom_point()
```

```{r}
library(corrr)
correlate(train)
correlate(train) |> autoplot()
correlate(train) |> network_plot()
```

# Feature Engineering

Outcome:

- target
- response
- class
- y
- label
- dependent variable

Input:

- independent variable
- features
- x
- covariates
- predictor

coefficients == weights (ML)

intercept == bias

inverse logit == sigmoid

Dummy variables == indicator variables == one hot encoded

```{r}
train |> count(Home)

train |> select(Home)

train |> select(Home) |> head(n=10) |> 
    model.matrix( ~ Home, data=_)
```

`{recipes}`

```{r}
library(recipes)

rec1 <- recipe(Status ~ ., data=train) |> 
    # not really needed for xgboost, but good for 
    # other methods
    themis::step_downsample(Status, under_ratio=1.2) |> 
    step_nzv(all_predictors()) |> 
    step_filter_missing(all_predictors(), threshold=.5) |> 
    step_unknown(
        all_nominal_predictors(), 
        new_level='missing'
    ) |> 
    step_impute_knn(all_numeric_predictors()) |> 
    step_normalize(all_numeric_predictors()) |> 
    # step_center() |> step_scale() |> 
    step_other(all_nominal_predictors(), other='misc') |> 
    step_novel(
        all_nominal_predictors(), new_level='unseen'
    ) |> 
    step_dummy(
        all_nominal_predictors(), 
        one_hot=TRUE
    )

rec1

rec1 |> prep()
rec1 |> prep() |> bake(new_data=NULL)
# rec1 |> prep() |> bake(new_data=NULL) |> View()
```


# Define the Model

xgboost

```{r}
library(parsnip)

linear_reg()
linear_reg() |> set_engine('lm')
linear_reg() |> set_engine('glmnet')
linear_reg() |> set_engine('stan')
linear_reg() |> set_engine('keras')
linear_reg() |> set_engine('spark')

rand_forest() |> set_engine('randomForest')
rand_forest() |> set_engine('ranger')
rand_forest() |> set_engine('spark')

boost_tree() |> set_engine('xgboost')
boost_tree(mode='classification') |> set_engine('xgboost')
```

```{r}
spec1 <- boost_tree(
    mode='classification',
    trees=100,
    tree_depth=4
) |> 
    set_engine('xgboost')
spec1
```


# Workflows

```{r}
library(workflows)
```

```{r}
flow1 <- workflow(preprocessor=rec1, spec=spec1)
flow1 <- workflow() |> 
    add_recipe(rec1) |> 
    add_model(spec1)

flow1
```

```{r}
fit1 <- fit(flow1, data=train)
fit1

# xgboost::xgb.train(
#     data=xgboost::xgb.DMatrix(
#         x_matrix, label=y_matrix
#     ),
#     nrounds=100, 
#     max_depth=4
# )
```


```{r}
fit1 |> summary()
fit1 |> extract_fit_engine() |> vip::vip()
```

```{r}
spec2 <- boost_tree(
    'classification', 
    trees=400, 
    tree_depth=4
) |> 
    set_engine('xgboost')

flow2 <- flow1 |> update_model(spec2)
flow2

fit2 <- fit(flow2, data=train)
fit2 |> extract_fit_engine() |> vip::vip()
```

# Evaluate the Model

Regression:

- root mean squared error
- mean absolute error

Classification

- accuracy
- log loss
- AUC


```{r}
library(yardstick)

loss_fn <- metric_set(roc_auc)
loss_fn
```

# Cross-Validation

Still using `{rsample}`

```{r}
set.seed(18002)
theCV <- vfold_cv(
    data=train, 
    v=5, 
    repeats=1, 
    strata='Status'
)

theCV
theCV$splits
theCV$splits[[1]] |> training()
```

```{r}
library(tune)
val1 <- fit_resamples(
    flow1, 
    resamples=theCV, 
    metrics=loss_fn
)

val2 <- fit_resamples(
    flow2, 
    resamples=theCV, 
    metrics=loss_fn
)

val1
val2
val2$.metrics[[1]]
val2$.metrics[[2]]

val1 |> collect_metrics()
val2 |> collect_metrics()
```

# Tuning

Still using `{tune}`

```{r}
spec3 <- boost_tree(
    'classification',
    trees=tune(),
    tree_depth=tune(),
    sample_size=0.8
) |> 
    set_engine('xgboost')

spec3
```

```{r}
rec3 <- recipe(Status ~ ., data=train) |> 
    # not really needed for xgboost, but good for 
    # other methods
    themis::step_downsample(Status, under_ratio=1.2) |> 
    step_nzv(all_predictors()) |> 
    step_filter_missing(
        all_predictors(), 
        threshold=tune()
    ) |> 
    step_unknown(
        all_nominal_predictors(), 
        new_level='missing'
    ) |> 
    step_impute_knn(all_numeric_predictors()) |> 
    step_normalize(all_numeric_predictors()) |> 
    # step_center() |> step_scale() |> 
    step_other(all_nominal_predictors(), other='misc') |> 
    step_novel(
        all_nominal_predictors(), new_level='unseen'
    ) |> 
    step_dummy(
        all_nominal_predictors(), 
        one_hot=TRUE
    )

rec3
```

```{r}
flow3 <- workflow() |> 
    add_recipe(rec3) |> 
    add_model(spec3)

flow3
```

```{r}
params <- flow3 |> hardhat::extract_parameter_set_dials()
params$object
```


```{r}
library(dials)

params3 <- params |> 
    update(
        trees=trees(c(50, 500)),
        tree_depth=tree_depth(c(2, 6)),
        threshold=threshold(c(0.4, 1))
    )
params3$object
```

```{r}
set.seed(99)
grid3 <- grid_latin_hypercube(x=params3, size=150)
grid3

grid3 <- grid3 |> 
  mutate(threshold=round(threshold, digits=2))
```

```{r}
# options(tidymodels.dark=TRUE)
```

```{r}
library(tictoc)
```

```{r}
library(parallel)
library(doFuture)
registerDoFuture()
cl <- makeCluster(6)
plan(cluster, workers = cl)
```


```{r}
tic(msg='tuning 3')
tuned3 <- tune_grid(
  flow3,
  resamples=theCV,
  grid=grid3,
  metrics=loss_fn,
  control=control_grid(
    verbose=TRUE,
    allow_par=TRUE
  )
)
toc(log=TRUE)
```

